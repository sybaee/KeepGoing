{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, Y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
    "X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000) (10, 60000)\n",
      "(784, 10000) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train / X_train.max()\n",
    "X_test = X_test / X_test.max()\n",
    "\n",
    "y_train = np.zeros((Y_train.size, int(Y_train.max()) + 1))\n",
    "y_train[np.arange(Y_train.size), Y_train.astype(np.int)] = 1.0\n",
    "\n",
    "X_train = X_train.T\n",
    "y_train = y_train.T\n",
    "X_test = X_test.T\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP():\n",
    "    def __init__(self, numClass, X_train, y_train, batch_size, hidden_size):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.W1 = np.random.randn(self.hidden_size, self.X_train.shape[0]) * np.sqrt(\n",
    "            2. / self.X_train.shape[0])\n",
    "        self.b1 = np.zeros((self.hidden_size, 1))\n",
    "        self.W2 = np.random.randn(numClass, self.hidden_size)* np.sqrt(2. / self.hidden_size)\n",
    "        self.b2 = np.zeros((numClass, 1))\n",
    "    \n",
    "    def softmax(self, X):\n",
    "        exponent = np.exp(X)\n",
    "        return exponent / np.sum(exponent, axis=0)\n",
    "\n",
    "    def ReLU(self, X):\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def gradReLU(self, X):\n",
    "        return (X > 0) * 1\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        self.Z1 = np.matmul(self.W1, X) + self.b1\n",
    "        self.A1 = self.ReLU(self.Z1)\n",
    "        self.Z2 = np.matmul(self.W2, self.A1) + self.b2\n",
    "        self.A2 = self.softmax(self.Z2)\n",
    "        \n",
    "    def crossEntropyLoss(self, y, y_hat, lamda):\n",
    "        m = y.shape[1]\n",
    "        loss = -(1 / m) * np.sum(y*np.log(y_hat)) + lamda / (2*m) * (np.sum(\n",
    "            self.W1**2) + np.sum(self.W2**2))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def back_prop(self, X, y): # to backpropagate the error\n",
    "        m = X.shape[1]\n",
    "        self.dZ2 = self.A2 - y\n",
    "        self.dW2 = (1./m) * np.matmul(self.dZ2, self.A1.T)\n",
    "        self.db2 = (1./m) * np.sum(self.dZ2, axis=1, keepdims=True)\n",
    "        self.dA1 = np.matmul(self.W2.T, self.dZ2)\n",
    "        self.dZ1 = self.dA1 * self.gradReLU(self.Z1)\n",
    "        self.dW1 = (1./m) * np.matmul(self.dZ1, X.T)\n",
    "        self.db1 = (1./m) * np.sum(self.dZ1)\n",
    "        \n",
    "    def update(self, lr, lamda):\n",
    "        self.W2 = self.W2 - lr * self.dW2 - (self.W2*lamda*lr) / self.batch_size\n",
    "        self.b2 = self.b2 - lr * self.db2\n",
    "        self.W1 = self.W1 - lr * self.dW1 - (self.W1*lamda*lr) / self.batch_size\n",
    "        self.b1 = self.b1 - lr * self.db1\n",
    "        \n",
    "    def predict(self, X):\n",
    "        Z1 = np.matmul(self.W1, X) + self.b1\n",
    "        A1 = self.ReLU(Z1)\n",
    "        Z2 = np.matmul(self.W2, A1) + self.b2\n",
    "        A2 = self.softmax(Z2)\n",
    "\n",
    "        return np.argmax(A2, axis=0)\n",
    "    \n",
    "    def train(self, epochs, lamda, lr):\n",
    "        for epoch in range(epochs):\n",
    "            splitIndex = np.random.permutation(self.X_train.shape[1])[:self.batch_size]\n",
    "            X = X_train[:, splitIndex]\n",
    "            y = y_train[:, splitIndex]\n",
    "            \n",
    "            self.feed_forward(X)\n",
    "\n",
    "            loss = self.crossEntropyLoss(y, self.A2, lamda)\n",
    "            print('Epoch {} \\tTrain Loss: {:.4f}'.format(epoch + 1, loss))\n",
    "            \n",
    "            self.back_prop(X, y)\n",
    "            self.update(lr, lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClass = 10\n",
    "epochs = 1000\n",
    "batch_size = 1000\n",
    "hidden_size = 512\n",
    "lr, lamda = 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \tTrain Loss: 3.4069\n",
      "Epoch 2 \tTrain Loss: 3.0018\n",
      "Epoch 3 \tTrain Loss: 3.0426\n",
      "Epoch 4 \tTrain Loss: 2.7175\n",
      "Epoch 5 \tTrain Loss: 2.9511\n",
      "Epoch 6 \tTrain Loss: 2.4118\n",
      "Epoch 7 \tTrain Loss: 2.3852\n",
      "Epoch 8 \tTrain Loss: 1.9909\n",
      "Epoch 9 \tTrain Loss: 2.3536\n",
      "Epoch 10 \tTrain Loss: 2.5766\n",
      "Epoch 11 \tTrain Loss: 2.6794\n",
      "Epoch 12 \tTrain Loss: 3.0046\n",
      "Epoch 13 \tTrain Loss: 2.6035\n",
      "Epoch 14 \tTrain Loss: 2.2470\n",
      "Epoch 15 \tTrain Loss: 1.9382\n",
      "Epoch 16 \tTrain Loss: 1.7520\n",
      "Epoch 17 \tTrain Loss: 2.3773\n",
      "Epoch 18 \tTrain Loss: 3.4125\n",
      "Epoch 19 \tTrain Loss: 2.7377\n",
      "Epoch 20 \tTrain Loss: 2.4722\n",
      "Epoch 21 \tTrain Loss: 2.1325\n",
      "Epoch 22 \tTrain Loss: 2.0716\n",
      "Epoch 23 \tTrain Loss: 2.0968\n",
      "Epoch 24 \tTrain Loss: 1.8480\n",
      "Epoch 25 \tTrain Loss: 1.8302\n",
      "Epoch 26 \tTrain Loss: 1.9495\n",
      "Epoch 27 \tTrain Loss: 2.1580\n",
      "Epoch 28 \tTrain Loss: 1.8652\n",
      "Epoch 29 \tTrain Loss: 1.8307\n",
      "Epoch 30 \tTrain Loss: 1.6016\n",
      "Epoch 31 \tTrain Loss: 1.4916\n",
      "Epoch 32 \tTrain Loss: 1.4790\n",
      "Epoch 33 \tTrain Loss: 1.5537\n",
      "Epoch 34 \tTrain Loss: 1.6845\n",
      "Epoch 35 \tTrain Loss: 1.6001\n",
      "Epoch 36 \tTrain Loss: 1.4706\n",
      "Epoch 37 \tTrain Loss: 1.3646\n",
      "Epoch 38 \tTrain Loss: 1.3716\n",
      "Epoch 39 \tTrain Loss: 1.3674\n",
      "Epoch 40 \tTrain Loss: 1.3689\n",
      "Epoch 41 \tTrain Loss: 1.4153\n",
      "Epoch 42 \tTrain Loss: 1.5988\n",
      "Epoch 43 \tTrain Loss: 1.9010\n",
      "Epoch 44 \tTrain Loss: 1.7623\n",
      "Epoch 45 \tTrain Loss: 1.4421\n",
      "Epoch 46 \tTrain Loss: 1.3496\n",
      "Epoch 47 \tTrain Loss: 1.3154\n",
      "Epoch 48 \tTrain Loss: 1.3692\n",
      "Epoch 49 \tTrain Loss: 1.4065\n",
      "Epoch 50 \tTrain Loss: 1.5425\n",
      "Epoch 51 \tTrain Loss: 1.4015\n",
      "Epoch 52 \tTrain Loss: 1.3033\n",
      "Epoch 53 \tTrain Loss: 1.2724\n",
      "Epoch 54 \tTrain Loss: 1.2934\n",
      "Epoch 55 \tTrain Loss: 1.2221\n",
      "Epoch 56 \tTrain Loss: 1.2387\n",
      "Epoch 57 \tTrain Loss: 1.2188\n",
      "Epoch 58 \tTrain Loss: 1.2254\n",
      "Epoch 59 \tTrain Loss: 1.2956\n",
      "Epoch 60 \tTrain Loss: 1.2190\n",
      "Epoch 61 \tTrain Loss: 1.1683\n",
      "Epoch 62 \tTrain Loss: 1.1802\n",
      "Epoch 63 \tTrain Loss: 1.1219\n",
      "Epoch 64 \tTrain Loss: 1.1538\n",
      "Epoch 65 \tTrain Loss: 1.1875\n",
      "Epoch 66 \tTrain Loss: 1.1265\n",
      "Epoch 67 \tTrain Loss: 1.1282\n",
      "Epoch 68 \tTrain Loss: 1.1564\n",
      "Epoch 69 \tTrain Loss: 1.1538\n",
      "Epoch 70 \tTrain Loss: 1.1114\n",
      "Epoch 71 \tTrain Loss: 1.1429\n",
      "Epoch 72 \tTrain Loss: 1.1575\n",
      "Epoch 73 \tTrain Loss: 1.1825\n",
      "Epoch 74 \tTrain Loss: 1.2101\n",
      "Epoch 75 \tTrain Loss: 1.1613\n",
      "Epoch 76 \tTrain Loss: 1.1518\n",
      "Epoch 77 \tTrain Loss: 1.1241\n",
      "Epoch 78 \tTrain Loss: 1.1317\n",
      "Epoch 79 \tTrain Loss: 1.1203\n",
      "Epoch 80 \tTrain Loss: 1.1167\n",
      "Epoch 81 \tTrain Loss: 1.0944\n",
      "Epoch 82 \tTrain Loss: 1.0741\n",
      "Epoch 83 \tTrain Loss: 1.0909\n",
      "Epoch 84 \tTrain Loss: 1.0528\n",
      "Epoch 85 \tTrain Loss: 1.0928\n",
      "Epoch 86 \tTrain Loss: 1.0888\n",
      "Epoch 87 \tTrain Loss: 1.0634\n",
      "Epoch 88 \tTrain Loss: 1.0590\n",
      "Epoch 89 \tTrain Loss: 1.0508\n",
      "Epoch 90 \tTrain Loss: 1.1063\n",
      "Epoch 91 \tTrain Loss: 1.1188\n",
      "Epoch 92 \tTrain Loss: 1.0608\n",
      "Epoch 93 \tTrain Loss: 1.1421\n",
      "Epoch 94 \tTrain Loss: 1.0902\n",
      "Epoch 95 \tTrain Loss: 1.0512\n",
      "Epoch 96 \tTrain Loss: 1.0374\n",
      "Epoch 97 \tTrain Loss: 1.0501\n",
      "Epoch 98 \tTrain Loss: 1.0323\n",
      "Epoch 99 \tTrain Loss: 1.0818\n",
      "Epoch 100 \tTrain Loss: 1.0688\n",
      "Epoch 101 \tTrain Loss: 1.0295\n",
      "Epoch 102 \tTrain Loss: 1.0357\n",
      "Epoch 103 \tTrain Loss: 0.9960\n",
      "Epoch 104 \tTrain Loss: 0.9849\n",
      "Epoch 105 \tTrain Loss: 1.0043\n",
      "Epoch 106 \tTrain Loss: 0.9705\n",
      "Epoch 107 \tTrain Loss: 1.0077\n",
      "Epoch 108 \tTrain Loss: 1.0132\n",
      "Epoch 109 \tTrain Loss: 0.9573\n",
      "Epoch 110 \tTrain Loss: 0.9968\n",
      "Epoch 111 \tTrain Loss: 1.0175\n",
      "Epoch 112 \tTrain Loss: 0.9781\n",
      "Epoch 113 \tTrain Loss: 0.9267\n",
      "Epoch 114 \tTrain Loss: 0.9393\n",
      "Epoch 115 \tTrain Loss: 0.9763\n",
      "Epoch 116 \tTrain Loss: 0.9348\n",
      "Epoch 117 \tTrain Loss: 0.9477\n",
      "Epoch 118 \tTrain Loss: 0.9627\n",
      "Epoch 119 \tTrain Loss: 0.9453\n",
      "Epoch 120 \tTrain Loss: 0.9395\n",
      "Epoch 121 \tTrain Loss: 0.9404\n",
      "Epoch 122 \tTrain Loss: 0.9468\n",
      "Epoch 123 \tTrain Loss: 0.9283\n",
      "Epoch 124 \tTrain Loss: 0.9593\n",
      "Epoch 125 \tTrain Loss: 0.9351\n",
      "Epoch 126 \tTrain Loss: 0.9475\n",
      "Epoch 127 \tTrain Loss: 0.9124\n",
      "Epoch 128 \tTrain Loss: 0.9091\n",
      "Epoch 129 \tTrain Loss: 0.9178\n",
      "Epoch 130 \tTrain Loss: 0.9241\n",
      "Epoch 131 \tTrain Loss: 0.8858\n",
      "Epoch 132 \tTrain Loss: 0.9164\n",
      "Epoch 133 \tTrain Loss: 0.9693\n",
      "Epoch 134 \tTrain Loss: 0.9297\n",
      "Epoch 135 \tTrain Loss: 0.9416\n",
      "Epoch 136 \tTrain Loss: 0.9588\n",
      "Epoch 137 \tTrain Loss: 0.9560\n",
      "Epoch 138 \tTrain Loss: 0.9133\n",
      "Epoch 139 \tTrain Loss: 0.9210\n",
      "Epoch 140 \tTrain Loss: 0.8804\n",
      "Epoch 141 \tTrain Loss: 0.8619\n",
      "Epoch 142 \tTrain Loss: 0.8581\n",
      "Epoch 143 \tTrain Loss: 0.8920\n",
      "Epoch 144 \tTrain Loss: 0.8665\n",
      "Epoch 145 \tTrain Loss: 0.8639\n",
      "Epoch 146 \tTrain Loss: 0.8650\n",
      "Epoch 147 \tTrain Loss: 0.8326\n",
      "Epoch 148 \tTrain Loss: 0.8844\n",
      "Epoch 149 \tTrain Loss: 0.8538\n",
      "Epoch 150 \tTrain Loss: 0.8807\n",
      "Epoch 151 \tTrain Loss: 0.8480\n",
      "Epoch 152 \tTrain Loss: 0.8569\n",
      "Epoch 153 \tTrain Loss: 0.8457\n",
      "Epoch 154 \tTrain Loss: 0.8275\n",
      "Epoch 155 \tTrain Loss: 0.8681\n",
      "Epoch 156 \tTrain Loss: 0.8326\n",
      "Epoch 157 \tTrain Loss: 0.8150\n",
      "Epoch 158 \tTrain Loss: 0.8739\n",
      "Epoch 159 \tTrain Loss: 0.9112\n",
      "Epoch 160 \tTrain Loss: 0.8468\n",
      "Epoch 161 \tTrain Loss: 0.8733\n",
      "Epoch 162 \tTrain Loss: 0.8323\n",
      "Epoch 163 \tTrain Loss: 0.8268\n",
      "Epoch 164 \tTrain Loss: 0.8721\n",
      "Epoch 165 \tTrain Loss: 0.8330\n",
      "Epoch 166 \tTrain Loss: 0.8493\n",
      "Epoch 167 \tTrain Loss: 0.8390\n",
      "Epoch 168 \tTrain Loss: 0.8002\n",
      "Epoch 169 \tTrain Loss: 0.8048\n",
      "Epoch 170 \tTrain Loss: 0.8044\n",
      "Epoch 171 \tTrain Loss: 0.8089\n",
      "Epoch 172 \tTrain Loss: 0.7940\n",
      "Epoch 173 \tTrain Loss: 0.7989\n",
      "Epoch 174 \tTrain Loss: 0.8010\n",
      "Epoch 175 \tTrain Loss: 0.8204\n",
      "Epoch 176 \tTrain Loss: 0.7613\n",
      "Epoch 177 \tTrain Loss: 0.7882\n",
      "Epoch 178 \tTrain Loss: 0.7625\n",
      "Epoch 179 \tTrain Loss: 0.7980\n",
      "Epoch 180 \tTrain Loss: 0.8053\n",
      "Epoch 181 \tTrain Loss: 0.8137\n",
      "Epoch 182 \tTrain Loss: 0.8545\n",
      "Epoch 183 \tTrain Loss: 0.8173\n",
      "Epoch 184 \tTrain Loss: 0.8084\n",
      "Epoch 185 \tTrain Loss: 0.8029\n",
      "Epoch 186 \tTrain Loss: 0.8113\n",
      "Epoch 187 \tTrain Loss: 0.8051\n",
      "Epoch 188 \tTrain Loss: 0.7708\n",
      "Epoch 189 \tTrain Loss: 0.7389\n",
      "Epoch 190 \tTrain Loss: 0.7793\n",
      "Epoch 191 \tTrain Loss: 0.7620\n",
      "Epoch 192 \tTrain Loss: 0.7330\n",
      "Epoch 193 \tTrain Loss: 0.7384\n",
      "Epoch 194 \tTrain Loss: 0.7423\n",
      "Epoch 195 \tTrain Loss: 0.7324\n",
      "Epoch 196 \tTrain Loss: 0.7515\n",
      "Epoch 197 \tTrain Loss: 0.7431\n",
      "Epoch 198 \tTrain Loss: 0.7223\n",
      "Epoch 199 \tTrain Loss: 0.7246\n",
      "Epoch 200 \tTrain Loss: 0.7773\n",
      "Epoch 201 \tTrain Loss: 0.7231\n",
      "Epoch 202 \tTrain Loss: 0.7114\n",
      "Epoch 203 \tTrain Loss: 0.7231\n",
      "Epoch 204 \tTrain Loss: 0.7255\n",
      "Epoch 205 \tTrain Loss: 0.7547\n",
      "Epoch 206 \tTrain Loss: 0.7063\n",
      "Epoch 207 \tTrain Loss: 0.7101\n",
      "Epoch 208 \tTrain Loss: 0.7103\n",
      "Epoch 209 \tTrain Loss: 0.7084\n",
      "Epoch 210 \tTrain Loss: 0.7805\n",
      "Epoch 211 \tTrain Loss: 0.7253\n",
      "Epoch 212 \tTrain Loss: 0.7159\n",
      "Epoch 213 \tTrain Loss: 0.7155\n",
      "Epoch 214 \tTrain Loss: 0.6990\n",
      "Epoch 215 \tTrain Loss: 0.6841\n",
      "Epoch 216 \tTrain Loss: 0.6924\n",
      "Epoch 217 \tTrain Loss: 0.7113\n",
      "Epoch 218 \tTrain Loss: 0.7030\n",
      "Epoch 219 \tTrain Loss: 0.7085\n",
      "Epoch 220 \tTrain Loss: 0.6971\n",
      "Epoch 221 \tTrain Loss: 0.7017\n",
      "Epoch 222 \tTrain Loss: 0.6807\n",
      "Epoch 223 \tTrain Loss: 0.6792\n",
      "Epoch 224 \tTrain Loss: 0.6852\n",
      "Epoch 225 \tTrain Loss: 0.6840\n",
      "Epoch 226 \tTrain Loss: 0.6811\n",
      "Epoch 227 \tTrain Loss: 0.6939\n",
      "Epoch 228 \tTrain Loss: 0.7105\n",
      "Epoch 229 \tTrain Loss: 0.6678\n",
      "Epoch 230 \tTrain Loss: 0.6920\n",
      "Epoch 231 \tTrain Loss: 0.6753\n",
      "Epoch 232 \tTrain Loss: 0.6924\n",
      "Epoch 233 \tTrain Loss: 0.6688\n",
      "Epoch 234 \tTrain Loss: 0.6838\n",
      "Epoch 235 \tTrain Loss: 0.6518\n",
      "Epoch 236 \tTrain Loss: 0.6653\n",
      "Epoch 237 \tTrain Loss: 0.6819\n",
      "Epoch 238 \tTrain Loss: 0.6455\n",
      "Epoch 239 \tTrain Loss: 0.6621\n",
      "Epoch 240 \tTrain Loss: 0.6348\n",
      "Epoch 241 \tTrain Loss: 0.6665\n",
      "Epoch 242 \tTrain Loss: 0.6812\n",
      "Epoch 243 \tTrain Loss: 0.6747\n",
      "Epoch 244 \tTrain Loss: 0.6497\n",
      "Epoch 245 \tTrain Loss: 0.6326\n",
      "Epoch 246 \tTrain Loss: 0.6503\n",
      "Epoch 247 \tTrain Loss: 0.6502\n",
      "Epoch 248 \tTrain Loss: 0.6154\n",
      "Epoch 249 \tTrain Loss: 0.6416\n",
      "Epoch 250 \tTrain Loss: 0.6463\n",
      "Epoch 251 \tTrain Loss: 0.6136\n",
      "Epoch 252 \tTrain Loss: 0.6130\n",
      "Epoch 253 \tTrain Loss: 0.6381\n",
      "Epoch 254 \tTrain Loss: 0.6364\n",
      "Epoch 255 \tTrain Loss: 0.6176\n",
      "Epoch 256 \tTrain Loss: 0.6613\n",
      "Epoch 257 \tTrain Loss: 0.6281\n",
      "Epoch 258 \tTrain Loss: 0.6259\n",
      "Epoch 259 \tTrain Loss: 0.6437\n",
      "Epoch 260 \tTrain Loss: 0.6097\n",
      "Epoch 261 \tTrain Loss: 0.6150\n",
      "Epoch 262 \tTrain Loss: 0.6150\n",
      "Epoch 263 \tTrain Loss: 0.5995\n",
      "Epoch 264 \tTrain Loss: 0.6181\n",
      "Epoch 265 \tTrain Loss: 0.6319\n",
      "Epoch 266 \tTrain Loss: 0.6498\n",
      "Epoch 267 \tTrain Loss: 0.6677\n",
      "Epoch 268 \tTrain Loss: 0.7410\n",
      "Epoch 269 \tTrain Loss: 0.8438\n",
      "Epoch 270 \tTrain Loss: 0.7746\n",
      "Epoch 271 \tTrain Loss: 0.6146\n",
      "Epoch 272 \tTrain Loss: 0.6295\n",
      "Epoch 273 \tTrain Loss: 0.5953\n",
      "Epoch 274 \tTrain Loss: 0.5953\n",
      "Epoch 275 \tTrain Loss: 0.5925\n",
      "Epoch 276 \tTrain Loss: 0.6168\n",
      "Epoch 277 \tTrain Loss: 0.6126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 \tTrain Loss: 0.6047\n",
      "Epoch 279 \tTrain Loss: 0.5630\n",
      "Epoch 280 \tTrain Loss: 0.6043\n",
      "Epoch 281 \tTrain Loss: 0.5943\n",
      "Epoch 282 \tTrain Loss: 0.5777\n",
      "Epoch 283 \tTrain Loss: 0.5837\n",
      "Epoch 284 \tTrain Loss: 0.5847\n",
      "Epoch 285 \tTrain Loss: 0.5877\n",
      "Epoch 286 \tTrain Loss: 0.6285\n",
      "Epoch 287 \tTrain Loss: 0.6222\n",
      "Epoch 288 \tTrain Loss: 0.5825\n",
      "Epoch 289 \tTrain Loss: 0.5818\n",
      "Epoch 290 \tTrain Loss: 0.5885\n",
      "Epoch 291 \tTrain Loss: 0.6047\n",
      "Epoch 292 \tTrain Loss: 0.5650\n",
      "Epoch 293 \tTrain Loss: 0.5628\n",
      "Epoch 294 \tTrain Loss: 0.5913\n",
      "Epoch 295 \tTrain Loss: 0.6159\n",
      "Epoch 296 \tTrain Loss: 0.5903\n",
      "Epoch 297 \tTrain Loss: 0.5664\n",
      "Epoch 298 \tTrain Loss: 0.5642\n",
      "Epoch 299 \tTrain Loss: 0.5580\n",
      "Epoch 300 \tTrain Loss: 0.5923\n",
      "Epoch 301 \tTrain Loss: 0.5510\n",
      "Epoch 302 \tTrain Loss: 0.5518\n",
      "Epoch 303 \tTrain Loss: 0.5683\n",
      "Epoch 304 \tTrain Loss: 0.5530\n",
      "Epoch 305 \tTrain Loss: 0.5517\n",
      "Epoch 306 \tTrain Loss: 0.5697\n",
      "Epoch 307 \tTrain Loss: 0.5747\n",
      "Epoch 308 \tTrain Loss: 0.5844\n",
      "Epoch 309 \tTrain Loss: 0.5501\n",
      "Epoch 310 \tTrain Loss: 0.5496\n",
      "Epoch 311 \tTrain Loss: 0.5132\n",
      "Epoch 312 \tTrain Loss: 0.5319\n",
      "Epoch 313 \tTrain Loss: 0.5521\n",
      "Epoch 314 \tTrain Loss: 0.5739\n",
      "Epoch 315 \tTrain Loss: 0.5196\n",
      "Epoch 316 \tTrain Loss: 0.5346\n",
      "Epoch 317 \tTrain Loss: 0.5465\n",
      "Epoch 318 \tTrain Loss: 0.5463\n",
      "Epoch 319 \tTrain Loss: 0.5517\n",
      "Epoch 320 \tTrain Loss: 0.5202\n",
      "Epoch 321 \tTrain Loss: 0.5593\n",
      "Epoch 322 \tTrain Loss: 0.5390\n",
      "Epoch 323 \tTrain Loss: 0.5345\n",
      "Epoch 324 \tTrain Loss: 0.5347\n",
      "Epoch 325 \tTrain Loss: 0.5278\n",
      "Epoch 326 \tTrain Loss: 0.5531\n",
      "Epoch 327 \tTrain Loss: 0.5261\n",
      "Epoch 328 \tTrain Loss: 0.5338\n",
      "Epoch 329 \tTrain Loss: 0.5274\n",
      "Epoch 330 \tTrain Loss: 0.5301\n",
      "Epoch 331 \tTrain Loss: 0.5406\n",
      "Epoch 332 \tTrain Loss: 0.5468\n",
      "Epoch 333 \tTrain Loss: 0.5608\n",
      "Epoch 334 \tTrain Loss: 0.5868\n",
      "Epoch 335 \tTrain Loss: 0.5576\n",
      "Epoch 336 \tTrain Loss: 0.5419\n",
      "Epoch 337 \tTrain Loss: 0.5294\n",
      "Epoch 338 \tTrain Loss: 0.5151\n",
      "Epoch 339 \tTrain Loss: 0.5111\n",
      "Epoch 340 \tTrain Loss: 0.5136\n",
      "Epoch 341 \tTrain Loss: 0.5246\n",
      "Epoch 342 \tTrain Loss: 0.5099\n",
      "Epoch 343 \tTrain Loss: 0.5395\n",
      "Epoch 344 \tTrain Loss: 0.5285\n",
      "Epoch 345 \tTrain Loss: 0.5345\n",
      "Epoch 346 \tTrain Loss: 0.5184\n",
      "Epoch 347 \tTrain Loss: 0.5352\n",
      "Epoch 348 \tTrain Loss: 0.5065\n",
      "Epoch 349 \tTrain Loss: 0.5047\n",
      "Epoch 350 \tTrain Loss: 0.4825\n",
      "Epoch 351 \tTrain Loss: 0.5050\n",
      "Epoch 352 \tTrain Loss: 0.4901\n",
      "Epoch 353 \tTrain Loss: 0.4875\n",
      "Epoch 354 \tTrain Loss: 0.4977\n",
      "Epoch 355 \tTrain Loss: 0.4939\n",
      "Epoch 356 \tTrain Loss: 0.4581\n",
      "Epoch 357 \tTrain Loss: 0.4922\n",
      "Epoch 358 \tTrain Loss: 0.5148\n",
      "Epoch 359 \tTrain Loss: 0.5074\n",
      "Epoch 360 \tTrain Loss: 0.4840\n",
      "Epoch 361 \tTrain Loss: 0.4956\n",
      "Epoch 362 \tTrain Loss: 0.4690\n",
      "Epoch 363 \tTrain Loss: 0.5049\n",
      "Epoch 364 \tTrain Loss: 0.5105\n",
      "Epoch 365 \tTrain Loss: 0.4650\n",
      "Epoch 366 \tTrain Loss: 0.4771\n",
      "Epoch 367 \tTrain Loss: 0.4959\n",
      "Epoch 368 \tTrain Loss: 0.4915\n",
      "Epoch 369 \tTrain Loss: 0.4718\n",
      "Epoch 370 \tTrain Loss: 0.4833\n",
      "Epoch 371 \tTrain Loss: 0.4788\n",
      "Epoch 372 \tTrain Loss: 0.4981\n",
      "Epoch 373 \tTrain Loss: 0.4691\n",
      "Epoch 374 \tTrain Loss: 0.4672\n",
      "Epoch 375 \tTrain Loss: 0.4906\n",
      "Epoch 376 \tTrain Loss: 0.4851\n",
      "Epoch 377 \tTrain Loss: 0.4554\n",
      "Epoch 378 \tTrain Loss: 0.4605\n",
      "Epoch 379 \tTrain Loss: 0.4741\n",
      "Epoch 380 \tTrain Loss: 0.4703\n",
      "Epoch 381 \tTrain Loss: 0.4562\n",
      "Epoch 382 \tTrain Loss: 0.4682\n",
      "Epoch 383 \tTrain Loss: 0.4674\n",
      "Epoch 384 \tTrain Loss: 0.4522\n",
      "Epoch 385 \tTrain Loss: 0.4709\n",
      "Epoch 386 \tTrain Loss: 0.4551\n",
      "Epoch 387 \tTrain Loss: 0.4704\n",
      "Epoch 388 \tTrain Loss: 0.4682\n",
      "Epoch 389 \tTrain Loss: 0.4822\n",
      "Epoch 390 \tTrain Loss: 0.4644\n",
      "Epoch 391 \tTrain Loss: 0.4433\n",
      "Epoch 392 \tTrain Loss: 0.4706\n",
      "Epoch 393 \tTrain Loss: 0.4768\n",
      "Epoch 394 \tTrain Loss: 0.4730\n",
      "Epoch 395 \tTrain Loss: 0.4995\n",
      "Epoch 396 \tTrain Loss: 0.4627\n",
      "Epoch 397 \tTrain Loss: 0.4787\n",
      "Epoch 398 \tTrain Loss: 0.5078\n",
      "Epoch 399 \tTrain Loss: 0.4641\n",
      "Epoch 400 \tTrain Loss: 0.4647\n",
      "Epoch 401 \tTrain Loss: 0.4720\n",
      "Epoch 402 \tTrain Loss: 0.4335\n",
      "Epoch 403 \tTrain Loss: 0.4638\n",
      "Epoch 404 \tTrain Loss: 0.4951\n",
      "Epoch 405 \tTrain Loss: 0.4559\n",
      "Epoch 406 \tTrain Loss: 0.4732\n",
      "Epoch 407 \tTrain Loss: 0.4586\n",
      "Epoch 408 \tTrain Loss: 0.4427\n",
      "Epoch 409 \tTrain Loss: 0.4342\n",
      "Epoch 410 \tTrain Loss: 0.4527\n",
      "Epoch 411 \tTrain Loss: 0.4585\n",
      "Epoch 412 \tTrain Loss: 0.4405\n",
      "Epoch 413 \tTrain Loss: 0.4160\n",
      "Epoch 414 \tTrain Loss: 0.4577\n",
      "Epoch 415 \tTrain Loss: 0.4446\n",
      "Epoch 416 \tTrain Loss: 0.4344\n",
      "Epoch 417 \tTrain Loss: 0.4398\n",
      "Epoch 418 \tTrain Loss: 0.4661\n",
      "Epoch 419 \tTrain Loss: 0.4402\n",
      "Epoch 420 \tTrain Loss: 0.4536\n",
      "Epoch 421 \tTrain Loss: 0.4244\n",
      "Epoch 422 \tTrain Loss: 0.4329\n",
      "Epoch 423 \tTrain Loss: 0.4285\n",
      "Epoch 424 \tTrain Loss: 0.4162\n",
      "Epoch 425 \tTrain Loss: 0.4642\n",
      "Epoch 426 \tTrain Loss: 0.4278\n",
      "Epoch 427 \tTrain Loss: 0.4436\n",
      "Epoch 428 \tTrain Loss: 0.4311\n",
      "Epoch 429 \tTrain Loss: 0.3964\n",
      "Epoch 430 \tTrain Loss: 0.4262\n",
      "Epoch 431 \tTrain Loss: 0.4151\n",
      "Epoch 432 \tTrain Loss: 0.4352\n",
      "Epoch 433 \tTrain Loss: 0.4333\n",
      "Epoch 434 \tTrain Loss: 0.4108\n",
      "Epoch 435 \tTrain Loss: 0.3923\n",
      "Epoch 436 \tTrain Loss: 0.4090\n",
      "Epoch 437 \tTrain Loss: 0.4146\n",
      "Epoch 438 \tTrain Loss: 0.4280\n",
      "Epoch 439 \tTrain Loss: 0.4372\n",
      "Epoch 440 \tTrain Loss: 0.4583\n",
      "Epoch 441 \tTrain Loss: 0.4581\n",
      "Epoch 442 \tTrain Loss: 0.4868\n",
      "Epoch 443 \tTrain Loss: 0.4916\n",
      "Epoch 444 \tTrain Loss: 0.4699\n",
      "Epoch 445 \tTrain Loss: 0.4499\n",
      "Epoch 446 \tTrain Loss: 0.4449\n",
      "Epoch 447 \tTrain Loss: 0.4141\n",
      "Epoch 448 \tTrain Loss: 0.4479\n",
      "Epoch 449 \tTrain Loss: 0.4068\n",
      "Epoch 450 \tTrain Loss: 0.4054\n",
      "Epoch 451 \tTrain Loss: 0.4147\n",
      "Epoch 452 \tTrain Loss: 0.4088\n",
      "Epoch 453 \tTrain Loss: 0.4130\n",
      "Epoch 454 \tTrain Loss: 0.4148\n",
      "Epoch 455 \tTrain Loss: 0.3889\n",
      "Epoch 456 \tTrain Loss: 0.3944\n",
      "Epoch 457 \tTrain Loss: 0.4165\n",
      "Epoch 458 \tTrain Loss: 0.4270\n",
      "Epoch 459 \tTrain Loss: 0.3918\n",
      "Epoch 460 \tTrain Loss: 0.4178\n",
      "Epoch 461 \tTrain Loss: 0.4174\n",
      "Epoch 462 \tTrain Loss: 0.4073\n",
      "Epoch 463 \tTrain Loss: 0.3882\n",
      "Epoch 464 \tTrain Loss: 0.4049\n",
      "Epoch 465 \tTrain Loss: 0.4014\n",
      "Epoch 466 \tTrain Loss: 0.3986\n",
      "Epoch 467 \tTrain Loss: 0.3788\n",
      "Epoch 468 \tTrain Loss: 0.3685\n",
      "Epoch 469 \tTrain Loss: 0.4045\n",
      "Epoch 470 \tTrain Loss: 0.3866\n",
      "Epoch 471 \tTrain Loss: 0.4146\n",
      "Epoch 472 \tTrain Loss: 0.4110\n",
      "Epoch 473 \tTrain Loss: 0.3918\n",
      "Epoch 474 \tTrain Loss: 0.4033\n",
      "Epoch 475 \tTrain Loss: 0.4057\n",
      "Epoch 476 \tTrain Loss: 0.3876\n",
      "Epoch 477 \tTrain Loss: 0.3985\n",
      "Epoch 478 \tTrain Loss: 0.4044\n",
      "Epoch 479 \tTrain Loss: 0.3969\n",
      "Epoch 480 \tTrain Loss: 0.4126\n",
      "Epoch 481 \tTrain Loss: 0.4167\n",
      "Epoch 482 \tTrain Loss: 0.4131\n",
      "Epoch 483 \tTrain Loss: 0.4313\n",
      "Epoch 484 \tTrain Loss: 0.4199\n",
      "Epoch 485 \tTrain Loss: 0.3904\n",
      "Epoch 486 \tTrain Loss: 0.3929\n",
      "Epoch 487 \tTrain Loss: 0.4148\n",
      "Epoch 488 \tTrain Loss: 0.3939\n",
      "Epoch 489 \tTrain Loss: 0.3845\n",
      "Epoch 490 \tTrain Loss: 0.3984\n",
      "Epoch 491 \tTrain Loss: 0.3748\n",
      "Epoch 492 \tTrain Loss: 0.4263\n",
      "Epoch 493 \tTrain Loss: 0.3927\n",
      "Epoch 494 \tTrain Loss: 0.3681\n",
      "Epoch 495 \tTrain Loss: 0.3939\n",
      "Epoch 496 \tTrain Loss: 0.3681\n",
      "Epoch 497 \tTrain Loss: 0.3792\n",
      "Epoch 498 \tTrain Loss: 0.3823\n",
      "Epoch 499 \tTrain Loss: 0.3809\n",
      "Epoch 500 \tTrain Loss: 0.3865\n",
      "Epoch 501 \tTrain Loss: 0.3683\n",
      "Epoch 502 \tTrain Loss: 0.3741\n",
      "Epoch 503 \tTrain Loss: 0.3604\n",
      "Epoch 504 \tTrain Loss: 0.3845\n",
      "Epoch 505 \tTrain Loss: 0.3835\n",
      "Epoch 506 \tTrain Loss: 0.3901\n",
      "Epoch 507 \tTrain Loss: 0.3729\n",
      "Epoch 508 \tTrain Loss: 0.3973\n",
      "Epoch 509 \tTrain Loss: 0.3599\n",
      "Epoch 510 \tTrain Loss: 0.4002\n",
      "Epoch 511 \tTrain Loss: 0.3668\n",
      "Epoch 512 \tTrain Loss: 0.3752\n",
      "Epoch 513 \tTrain Loss: 0.3593\n",
      "Epoch 514 \tTrain Loss: 0.3725\n",
      "Epoch 515 \tTrain Loss: 0.3680\n",
      "Epoch 516 \tTrain Loss: 0.3726\n",
      "Epoch 517 \tTrain Loss: 0.3611\n",
      "Epoch 518 \tTrain Loss: 0.3736\n",
      "Epoch 519 \tTrain Loss: 0.3659\n",
      "Epoch 520 \tTrain Loss: 0.3682\n",
      "Epoch 521 \tTrain Loss: 0.3635\n",
      "Epoch 522 \tTrain Loss: 0.3623\n",
      "Epoch 523 \tTrain Loss: 0.3622\n",
      "Epoch 524 \tTrain Loss: 0.3398\n",
      "Epoch 525 \tTrain Loss: 0.3762\n",
      "Epoch 526 \tTrain Loss: 0.3400\n",
      "Epoch 527 \tTrain Loss: 0.3581\n",
      "Epoch 528 \tTrain Loss: 0.3579\n",
      "Epoch 529 \tTrain Loss: 0.3461\n",
      "Epoch 530 \tTrain Loss: 0.3709\n",
      "Epoch 531 \tTrain Loss: 0.3782\n",
      "Epoch 532 \tTrain Loss: 0.3700\n",
      "Epoch 533 \tTrain Loss: 0.3882\n",
      "Epoch 534 \tTrain Loss: 0.3846\n",
      "Epoch 535 \tTrain Loss: 0.3547\n",
      "Epoch 536 \tTrain Loss: 0.3675\n",
      "Epoch 537 \tTrain Loss: 0.3865\n",
      "Epoch 538 \tTrain Loss: 0.3607\n",
      "Epoch 539 \tTrain Loss: 0.3603\n",
      "Epoch 540 \tTrain Loss: 0.3736\n",
      "Epoch 541 \tTrain Loss: 0.3647\n",
      "Epoch 542 \tTrain Loss: 0.3288\n",
      "Epoch 543 \tTrain Loss: 0.3482\n",
      "Epoch 544 \tTrain Loss: 0.3592\n",
      "Epoch 545 \tTrain Loss: 0.3564\n",
      "Epoch 546 \tTrain Loss: 0.3618\n",
      "Epoch 547 \tTrain Loss: 0.3651\n",
      "Epoch 548 \tTrain Loss: 0.3828\n",
      "Epoch 549 \tTrain Loss: 0.3448\n",
      "Epoch 550 \tTrain Loss: 0.3724\n",
      "Epoch 551 \tTrain Loss: 0.3562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 552 \tTrain Loss: 0.3794\n",
      "Epoch 553 \tTrain Loss: 0.3607\n",
      "Epoch 554 \tTrain Loss: 0.3649\n",
      "Epoch 555 \tTrain Loss: 0.3707\n",
      "Epoch 556 \tTrain Loss: 0.3463\n",
      "Epoch 557 \tTrain Loss: 0.3334\n",
      "Epoch 558 \tTrain Loss: 0.3440\n",
      "Epoch 559 \tTrain Loss: 0.3508\n",
      "Epoch 560 \tTrain Loss: 0.3592\n",
      "Epoch 561 \tTrain Loss: 0.3247\n",
      "Epoch 562 \tTrain Loss: 0.3211\n",
      "Epoch 563 \tTrain Loss: 0.3418\n",
      "Epoch 564 \tTrain Loss: 0.3524\n",
      "Epoch 565 \tTrain Loss: 0.3349\n",
      "Epoch 566 \tTrain Loss: 0.3513\n",
      "Epoch 567 \tTrain Loss: 0.3387\n",
      "Epoch 568 \tTrain Loss: 0.3489\n",
      "Epoch 569 \tTrain Loss: 0.3505\n",
      "Epoch 570 \tTrain Loss: 0.3260\n",
      "Epoch 571 \tTrain Loss: 0.3301\n",
      "Epoch 572 \tTrain Loss: 0.3642\n",
      "Epoch 573 \tTrain Loss: 0.3319\n",
      "Epoch 574 \tTrain Loss: 0.3549\n",
      "Epoch 575 \tTrain Loss: 0.3740\n",
      "Epoch 576 \tTrain Loss: 0.3671\n",
      "Epoch 577 \tTrain Loss: 0.3624\n",
      "Epoch 578 \tTrain Loss: 0.3901\n",
      "Epoch 579 \tTrain Loss: 0.3733\n",
      "Epoch 580 \tTrain Loss: 0.3887\n",
      "Epoch 581 \tTrain Loss: 0.3331\n",
      "Epoch 582 \tTrain Loss: 0.3221\n",
      "Epoch 583 \tTrain Loss: 0.3360\n",
      "Epoch 584 \tTrain Loss: 0.3218\n",
      "Epoch 585 \tTrain Loss: 0.3409\n",
      "Epoch 586 \tTrain Loss: 0.3365\n",
      "Epoch 587 \tTrain Loss: 0.3474\n",
      "Epoch 588 \tTrain Loss: 0.3398\n",
      "Epoch 589 \tTrain Loss: 0.3300\n",
      "Epoch 590 \tTrain Loss: 0.3602\n",
      "Epoch 591 \tTrain Loss: 0.3189\n",
      "Epoch 592 \tTrain Loss: 0.3296\n",
      "Epoch 593 \tTrain Loss: 0.3462\n",
      "Epoch 594 \tTrain Loss: 0.3250\n",
      "Epoch 595 \tTrain Loss: 0.3191\n",
      "Epoch 596 \tTrain Loss: 0.3260\n",
      "Epoch 597 \tTrain Loss: 0.3451\n",
      "Epoch 598 \tTrain Loss: 0.3386\n",
      "Epoch 599 \tTrain Loss: 0.3415\n",
      "Epoch 600 \tTrain Loss: 0.3274\n",
      "Epoch 601 \tTrain Loss: 0.3420\n",
      "Epoch 602 \tTrain Loss: 0.3253\n",
      "Epoch 603 \tTrain Loss: 0.3450\n",
      "Epoch 604 \tTrain Loss: 0.3330\n",
      "Epoch 605 \tTrain Loss: 0.3407\n",
      "Epoch 606 \tTrain Loss: 0.3534\n",
      "Epoch 607 \tTrain Loss: 0.3538\n",
      "Epoch 608 \tTrain Loss: 0.3380\n",
      "Epoch 609 \tTrain Loss: 0.3559\n",
      "Epoch 610 \tTrain Loss: 0.3360\n",
      "Epoch 611 \tTrain Loss: 0.3814\n",
      "Epoch 612 \tTrain Loss: 0.4096\n",
      "Epoch 613 \tTrain Loss: 0.3728\n",
      "Epoch 614 \tTrain Loss: 0.3810\n",
      "Epoch 615 \tTrain Loss: 0.3791\n",
      "Epoch 616 \tTrain Loss: 0.3515\n",
      "Epoch 617 \tTrain Loss: 0.3621\n",
      "Epoch 618 \tTrain Loss: 0.3274\n",
      "Epoch 619 \tTrain Loss: 0.3580\n",
      "Epoch 620 \tTrain Loss: 0.4203\n",
      "Epoch 621 \tTrain Loss: 0.4466\n",
      "Epoch 622 \tTrain Loss: 0.3435\n",
      "Epoch 623 \tTrain Loss: 0.3504\n",
      "Epoch 624 \tTrain Loss: 0.3364\n",
      "Epoch 625 \tTrain Loss: 0.2999\n",
      "Epoch 626 \tTrain Loss: 0.3081\n",
      "Epoch 627 \tTrain Loss: 0.3091\n",
      "Epoch 628 \tTrain Loss: 0.3224\n",
      "Epoch 629 \tTrain Loss: 0.3214\n",
      "Epoch 630 \tTrain Loss: 0.3198\n",
      "Epoch 631 \tTrain Loss: 0.3188\n",
      "Epoch 632 \tTrain Loss: 0.3099\n",
      "Epoch 633 \tTrain Loss: 0.2988\n",
      "Epoch 634 \tTrain Loss: 0.3150\n",
      "Epoch 635 \tTrain Loss: 0.3131\n",
      "Epoch 636 \tTrain Loss: 0.3486\n",
      "Epoch 637 \tTrain Loss: 0.3063\n",
      "Epoch 638 \tTrain Loss: 0.3118\n",
      "Epoch 639 \tTrain Loss: 0.3088\n",
      "Epoch 640 \tTrain Loss: 0.3060\n",
      "Epoch 641 \tTrain Loss: 0.3127\n",
      "Epoch 642 \tTrain Loss: 0.3014\n",
      "Epoch 643 \tTrain Loss: 0.3100\n",
      "Epoch 644 \tTrain Loss: 0.3243\n",
      "Epoch 645 \tTrain Loss: 0.3388\n",
      "Epoch 646 \tTrain Loss: 0.3073\n",
      "Epoch 647 \tTrain Loss: 0.3056\n",
      "Epoch 648 \tTrain Loss: 0.3181\n",
      "Epoch 649 \tTrain Loss: 0.3304\n",
      "Epoch 650 \tTrain Loss: 0.3216\n",
      "Epoch 651 \tTrain Loss: 0.3226\n",
      "Epoch 652 \tTrain Loss: 0.3094\n",
      "Epoch 653 \tTrain Loss: 0.3191\n",
      "Epoch 654 \tTrain Loss: 0.3228\n",
      "Epoch 655 \tTrain Loss: 0.3091\n",
      "Epoch 656 \tTrain Loss: 0.3106\n",
      "Epoch 657 \tTrain Loss: 0.3270\n",
      "Epoch 658 \tTrain Loss: 0.3248\n",
      "Epoch 659 \tTrain Loss: 0.3293\n",
      "Epoch 660 \tTrain Loss: 0.2908\n",
      "Epoch 661 \tTrain Loss: 0.2985\n",
      "Epoch 662 \tTrain Loss: 0.3039\n",
      "Epoch 663 \tTrain Loss: 0.3304\n",
      "Epoch 664 \tTrain Loss: 0.3206\n",
      "Epoch 665 \tTrain Loss: 0.3114\n",
      "Epoch 666 \tTrain Loss: 0.3189\n",
      "Epoch 667 \tTrain Loss: 0.3214\n",
      "Epoch 668 \tTrain Loss: 0.2897\n",
      "Epoch 669 \tTrain Loss: 0.2960\n",
      "Epoch 670 \tTrain Loss: 0.2942\n",
      "Epoch 671 \tTrain Loss: 0.3189\n",
      "Epoch 672 \tTrain Loss: 0.3140\n",
      "Epoch 673 \tTrain Loss: 0.3345\n",
      "Epoch 674 \tTrain Loss: 0.3194\n",
      "Epoch 675 \tTrain Loss: 0.3274\n",
      "Epoch 676 \tTrain Loss: 0.3186\n",
      "Epoch 677 \tTrain Loss: 0.3065\n",
      "Epoch 678 \tTrain Loss: 0.3293\n",
      "Epoch 679 \tTrain Loss: 0.3040\n",
      "Epoch 680 \tTrain Loss: 0.3151\n",
      "Epoch 681 \tTrain Loss: 0.3283\n",
      "Epoch 682 \tTrain Loss: 0.3503\n",
      "Epoch 683 \tTrain Loss: 0.3124\n",
      "Epoch 684 \tTrain Loss: 0.3044\n",
      "Epoch 685 \tTrain Loss: 0.2869\n",
      "Epoch 686 \tTrain Loss: 0.3099\n",
      "Epoch 687 \tTrain Loss: 0.3064\n",
      "Epoch 688 \tTrain Loss: 0.3060\n",
      "Epoch 689 \tTrain Loss: 0.2772\n",
      "Epoch 690 \tTrain Loss: 0.3111\n",
      "Epoch 691 \tTrain Loss: 0.2893\n",
      "Epoch 692 \tTrain Loss: 0.3053\n",
      "Epoch 693 \tTrain Loss: 0.2947\n",
      "Epoch 694 \tTrain Loss: 0.2740\n",
      "Epoch 695 \tTrain Loss: 0.2985\n",
      "Epoch 696 \tTrain Loss: 0.2920\n",
      "Epoch 697 \tTrain Loss: 0.2943\n",
      "Epoch 698 \tTrain Loss: 0.2986\n",
      "Epoch 699 \tTrain Loss: 0.2900\n",
      "Epoch 700 \tTrain Loss: 0.3043\n",
      "Epoch 701 \tTrain Loss: 0.2828\n",
      "Epoch 702 \tTrain Loss: 0.2828\n",
      "Epoch 703 \tTrain Loss: 0.2955\n",
      "Epoch 704 \tTrain Loss: 0.3139\n",
      "Epoch 705 \tTrain Loss: 0.2829\n",
      "Epoch 706 \tTrain Loss: 0.3067\n",
      "Epoch 707 \tTrain Loss: 0.2987\n",
      "Epoch 708 \tTrain Loss: 0.2837\n",
      "Epoch 709 \tTrain Loss: 0.3078\n",
      "Epoch 710 \tTrain Loss: 0.2984\n",
      "Epoch 711 \tTrain Loss: 0.3111\n",
      "Epoch 712 \tTrain Loss: 0.2815\n",
      "Epoch 713 \tTrain Loss: 0.2822\n",
      "Epoch 714 \tTrain Loss: 0.2802\n",
      "Epoch 715 \tTrain Loss: 0.2706\n",
      "Epoch 716 \tTrain Loss: 0.2750\n",
      "Epoch 717 \tTrain Loss: 0.2755\n",
      "Epoch 718 \tTrain Loss: 0.3002\n",
      "Epoch 719 \tTrain Loss: 0.3015\n",
      "Epoch 720 \tTrain Loss: 0.2870\n",
      "Epoch 721 \tTrain Loss: 0.2883\n",
      "Epoch 722 \tTrain Loss: 0.2913\n",
      "Epoch 723 \tTrain Loss: 0.2988\n",
      "Epoch 724 \tTrain Loss: 0.2971\n",
      "Epoch 725 \tTrain Loss: 0.3052\n",
      "Epoch 726 \tTrain Loss: 0.2968\n",
      "Epoch 727 \tTrain Loss: 0.2883\n",
      "Epoch 728 \tTrain Loss: 0.3188\n",
      "Epoch 729 \tTrain Loss: 0.3255\n",
      "Epoch 730 \tTrain Loss: 0.3124\n",
      "Epoch 731 \tTrain Loss: 0.3126\n",
      "Epoch 732 \tTrain Loss: 0.2890\n",
      "Epoch 733 \tTrain Loss: 0.3101\n",
      "Epoch 734 \tTrain Loss: 0.2986\n",
      "Epoch 735 \tTrain Loss: 0.3068\n",
      "Epoch 736 \tTrain Loss: 0.3063\n",
      "Epoch 737 \tTrain Loss: 0.3077\n",
      "Epoch 738 \tTrain Loss: 0.3039\n",
      "Epoch 739 \tTrain Loss: 0.2983\n",
      "Epoch 740 \tTrain Loss: 0.2976\n",
      "Epoch 741 \tTrain Loss: 0.3050\n",
      "Epoch 742 \tTrain Loss: 0.2529\n",
      "Epoch 743 \tTrain Loss: 0.2925\n",
      "Epoch 744 \tTrain Loss: 0.2863\n",
      "Epoch 745 \tTrain Loss: 0.2884\n",
      "Epoch 746 \tTrain Loss: 0.2941\n",
      "Epoch 747 \tTrain Loss: 0.2766\n",
      "Epoch 748 \tTrain Loss: 0.2959\n",
      "Epoch 749 \tTrain Loss: 0.2899\n",
      "Epoch 750 \tTrain Loss: 0.2665\n",
      "Epoch 751 \tTrain Loss: 0.2691\n",
      "Epoch 752 \tTrain Loss: 0.2951\n",
      "Epoch 753 \tTrain Loss: 0.2793\n",
      "Epoch 754 \tTrain Loss: 0.2880\n",
      "Epoch 755 \tTrain Loss: 0.2885\n",
      "Epoch 756 \tTrain Loss: 0.2799\n",
      "Epoch 757 \tTrain Loss: 0.2893\n",
      "Epoch 758 \tTrain Loss: 0.2820\n",
      "Epoch 759 \tTrain Loss: 0.2710\n",
      "Epoch 760 \tTrain Loss: 0.2762\n",
      "Epoch 761 \tTrain Loss: 0.2710\n",
      "Epoch 762 \tTrain Loss: 0.2762\n",
      "Epoch 763 \tTrain Loss: 0.2876\n",
      "Epoch 764 \tTrain Loss: 0.2725\n",
      "Epoch 765 \tTrain Loss: 0.2621\n",
      "Epoch 766 \tTrain Loss: 0.2783\n",
      "Epoch 767 \tTrain Loss: 0.2897\n",
      "Epoch 768 \tTrain Loss: 0.2957\n",
      "Epoch 769 \tTrain Loss: 0.2747\n",
      "Epoch 770 \tTrain Loss: 0.2728\n",
      "Epoch 771 \tTrain Loss: 0.2812\n",
      "Epoch 772 \tTrain Loss: 0.2834\n",
      "Epoch 773 \tTrain Loss: 0.2742\n",
      "Epoch 774 \tTrain Loss: 0.2511\n",
      "Epoch 775 \tTrain Loss: 0.2702\n",
      "Epoch 776 \tTrain Loss: 0.2542\n",
      "Epoch 777 \tTrain Loss: 0.2667\n",
      "Epoch 778 \tTrain Loss: 0.2811\n",
      "Epoch 779 \tTrain Loss: 0.2983\n",
      "Epoch 780 \tTrain Loss: 0.2940\n",
      "Epoch 781 \tTrain Loss: 0.2879\n",
      "Epoch 782 \tTrain Loss: 0.3005\n",
      "Epoch 783 \tTrain Loss: 0.3110\n",
      "Epoch 784 \tTrain Loss: 0.2873\n",
      "Epoch 785 \tTrain Loss: 0.2714\n",
      "Epoch 786 \tTrain Loss: 0.2966\n",
      "Epoch 787 \tTrain Loss: 0.3035\n",
      "Epoch 788 \tTrain Loss: 0.2969\n",
      "Epoch 789 \tTrain Loss: 0.3067\n",
      "Epoch 790 \tTrain Loss: 0.2996\n",
      "Epoch 791 \tTrain Loss: 0.2747\n",
      "Epoch 792 \tTrain Loss: 0.2734\n",
      "Epoch 793 \tTrain Loss: 0.2612\n",
      "Epoch 794 \tTrain Loss: 0.2819\n",
      "Epoch 795 \tTrain Loss: 0.2676\n",
      "Epoch 796 \tTrain Loss: 0.2636\n",
      "Epoch 797 \tTrain Loss: 0.2669\n",
      "Epoch 798 \tTrain Loss: 0.2834\n",
      "Epoch 799 \tTrain Loss: 0.2749\n",
      "Epoch 800 \tTrain Loss: 0.2720\n",
      "Epoch 801 \tTrain Loss: 0.2550\n",
      "Epoch 802 \tTrain Loss: 0.2752\n",
      "Epoch 803 \tTrain Loss: 0.2649\n",
      "Epoch 804 \tTrain Loss: 0.2695\n",
      "Epoch 805 \tTrain Loss: 0.2843\n",
      "Epoch 806 \tTrain Loss: 0.2751\n",
      "Epoch 807 \tTrain Loss: 0.2697\n",
      "Epoch 808 \tTrain Loss: 0.2859\n",
      "Epoch 809 \tTrain Loss: 0.2971\n",
      "Epoch 810 \tTrain Loss: 0.3121\n",
      "Epoch 811 \tTrain Loss: 0.3085\n",
      "Epoch 812 \tTrain Loss: 0.3033\n",
      "Epoch 813 \tTrain Loss: 0.3189\n",
      "Epoch 814 \tTrain Loss: 0.3045\n",
      "Epoch 815 \tTrain Loss: 0.2802\n",
      "Epoch 816 \tTrain Loss: 0.2625\n",
      "Epoch 817 \tTrain Loss: 0.2605\n",
      "Epoch 818 \tTrain Loss: 0.2652\n",
      "Epoch 819 \tTrain Loss: 0.2729\n",
      "Epoch 820 \tTrain Loss: 0.2806\n",
      "Epoch 821 \tTrain Loss: 0.2782\n",
      "Epoch 822 \tTrain Loss: 0.2822\n",
      "Epoch 823 \tTrain Loss: 0.2675\n",
      "Epoch 824 \tTrain Loss: 0.2448\n",
      "Epoch 825 \tTrain Loss: 0.2873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 826 \tTrain Loss: 0.2699\n",
      "Epoch 827 \tTrain Loss: 0.2764\n",
      "Epoch 828 \tTrain Loss: 0.2570\n",
      "Epoch 829 \tTrain Loss: 0.2747\n",
      "Epoch 830 \tTrain Loss: 0.2791\n",
      "Epoch 831 \tTrain Loss: 0.2621\n",
      "Epoch 832 \tTrain Loss: 0.2581\n",
      "Epoch 833 \tTrain Loss: 0.2686\n",
      "Epoch 834 \tTrain Loss: 0.2882\n",
      "Epoch 835 \tTrain Loss: 0.2636\n",
      "Epoch 836 \tTrain Loss: 0.2723\n",
      "Epoch 837 \tTrain Loss: 0.2590\n",
      "Epoch 838 \tTrain Loss: 0.2665\n",
      "Epoch 839 \tTrain Loss: 0.2770\n",
      "Epoch 840 \tTrain Loss: 0.2958\n",
      "Epoch 841 \tTrain Loss: 0.2544\n",
      "Epoch 842 \tTrain Loss: 0.2723\n",
      "Epoch 843 \tTrain Loss: 0.2748\n",
      "Epoch 844 \tTrain Loss: 0.2802\n",
      "Epoch 845 \tTrain Loss: 0.2541\n",
      "Epoch 846 \tTrain Loss: 0.2676\n",
      "Epoch 847 \tTrain Loss: 0.2877\n",
      "Epoch 848 \tTrain Loss: 0.2786\n",
      "Epoch 849 \tTrain Loss: 0.2691\n",
      "Epoch 850 \tTrain Loss: 0.2879\n",
      "Epoch 851 \tTrain Loss: 0.2689\n",
      "Epoch 852 \tTrain Loss: 0.2791\n",
      "Epoch 853 \tTrain Loss: 0.3068\n",
      "Epoch 854 \tTrain Loss: 0.2825\n",
      "Epoch 855 \tTrain Loss: 0.2812\n",
      "Epoch 856 \tTrain Loss: 0.2771\n",
      "Epoch 857 \tTrain Loss: 0.2930\n",
      "Epoch 858 \tTrain Loss: 0.2738\n",
      "Epoch 859 \tTrain Loss: 0.2683\n",
      "Epoch 860 \tTrain Loss: 0.2678\n",
      "Epoch 861 \tTrain Loss: 0.2542\n",
      "Epoch 862 \tTrain Loss: 0.2682\n",
      "Epoch 863 \tTrain Loss: 0.2849\n",
      "Epoch 864 \tTrain Loss: 0.2803\n",
      "Epoch 865 \tTrain Loss: 0.2916\n",
      "Epoch 866 \tTrain Loss: 0.2829\n",
      "Epoch 867 \tTrain Loss: 0.2909\n",
      "Epoch 868 \tTrain Loss: 0.2657\n",
      "Epoch 869 \tTrain Loss: 0.2738\n",
      "Epoch 870 \tTrain Loss: 0.2596\n",
      "Epoch 871 \tTrain Loss: 0.2679\n",
      "Epoch 872 \tTrain Loss: 0.2593\n",
      "Epoch 873 \tTrain Loss: 0.2728\n",
      "Epoch 874 \tTrain Loss: 0.2634\n",
      "Epoch 875 \tTrain Loss: 0.2663\n",
      "Epoch 876 \tTrain Loss: 0.2343\n",
      "Epoch 877 \tTrain Loss: 0.2507\n",
      "Epoch 878 \tTrain Loss: 0.2597\n",
      "Epoch 879 \tTrain Loss: 0.2554\n",
      "Epoch 880 \tTrain Loss: 0.2673\n",
      "Epoch 881 \tTrain Loss: 0.2663\n",
      "Epoch 882 \tTrain Loss: 0.2565\n",
      "Epoch 883 \tTrain Loss: 0.2445\n",
      "Epoch 884 \tTrain Loss: 0.2656\n",
      "Epoch 885 \tTrain Loss: 0.2697\n",
      "Epoch 886 \tTrain Loss: 0.2764\n",
      "Epoch 887 \tTrain Loss: 0.2942\n",
      "Epoch 888 \tTrain Loss: 0.2585\n",
      "Epoch 889 \tTrain Loss: 0.2686\n",
      "Epoch 890 \tTrain Loss: 0.2602\n",
      "Epoch 891 \tTrain Loss: 0.2519\n",
      "Epoch 892 \tTrain Loss: 0.2591\n",
      "Epoch 893 \tTrain Loss: 0.2637\n",
      "Epoch 894 \tTrain Loss: 0.2736\n",
      "Epoch 895 \tTrain Loss: 0.2590\n",
      "Epoch 896 \tTrain Loss: 0.2837\n",
      "Epoch 897 \tTrain Loss: 0.2769\n",
      "Epoch 898 \tTrain Loss: 0.2666\n",
      "Epoch 899 \tTrain Loss: 0.2478\n",
      "Epoch 900 \tTrain Loss: 0.2498\n",
      "Epoch 901 \tTrain Loss: 0.2580\n",
      "Epoch 902 \tTrain Loss: 0.2521\n",
      "Epoch 903 \tTrain Loss: 0.2657\n",
      "Epoch 904 \tTrain Loss: 0.2404\n",
      "Epoch 905 \tTrain Loss: 0.2619\n",
      "Epoch 906 \tTrain Loss: 0.2745\n",
      "Epoch 907 \tTrain Loss: 0.2600\n",
      "Epoch 908 \tTrain Loss: 0.2584\n",
      "Epoch 909 \tTrain Loss: 0.2549\n",
      "Epoch 910 \tTrain Loss: 0.2437\n",
      "Epoch 911 \tTrain Loss: 0.2700\n",
      "Epoch 912 \tTrain Loss: 0.2899\n",
      "Epoch 913 \tTrain Loss: 0.2519\n",
      "Epoch 914 \tTrain Loss: 0.2874\n",
      "Epoch 915 \tTrain Loss: 0.2684\n",
      "Epoch 916 \tTrain Loss: 0.2640\n",
      "Epoch 917 \tTrain Loss: 0.2477\n",
      "Epoch 918 \tTrain Loss: 0.2580\n",
      "Epoch 919 \tTrain Loss: 0.2447\n",
      "Epoch 920 \tTrain Loss: 0.2550\n",
      "Epoch 921 \tTrain Loss: 0.2733\n",
      "Epoch 922 \tTrain Loss: 0.2575\n",
      "Epoch 923 \tTrain Loss: 0.2561\n",
      "Epoch 924 \tTrain Loss: 0.2561\n",
      "Epoch 925 \tTrain Loss: 0.2682\n",
      "Epoch 926 \tTrain Loss: 0.2482\n",
      "Epoch 927 \tTrain Loss: 0.2805\n",
      "Epoch 928 \tTrain Loss: 0.2597\n",
      "Epoch 929 \tTrain Loss: 0.2727\n",
      "Epoch 930 \tTrain Loss: 0.2443\n",
      "Epoch 931 \tTrain Loss: 0.2497\n",
      "Epoch 932 \tTrain Loss: 0.2643\n",
      "Epoch 933 \tTrain Loss: 0.2457\n",
      "Epoch 934 \tTrain Loss: 0.2557\n",
      "Epoch 935 \tTrain Loss: 0.2581\n",
      "Epoch 936 \tTrain Loss: 0.2496\n",
      "Epoch 937 \tTrain Loss: 0.2607\n",
      "Epoch 938 \tTrain Loss: 0.2595\n",
      "Epoch 939 \tTrain Loss: 0.2491\n",
      "Epoch 940 \tTrain Loss: 0.2724\n",
      "Epoch 941 \tTrain Loss: 0.2770\n",
      "Epoch 942 \tTrain Loss: 0.2409\n",
      "Epoch 943 \tTrain Loss: 0.2403\n",
      "Epoch 944 \tTrain Loss: 0.2741\n",
      "Epoch 945 \tTrain Loss: 0.3011\n",
      "Epoch 946 \tTrain Loss: 0.2978\n",
      "Epoch 947 \tTrain Loss: 0.2884\n",
      "Epoch 948 \tTrain Loss: 0.3164\n",
      "Epoch 949 \tTrain Loss: 0.3272\n",
      "Epoch 950 \tTrain Loss: 0.4001\n",
      "Epoch 951 \tTrain Loss: 0.6159\n",
      "Epoch 952 \tTrain Loss: 0.3642\n",
      "Epoch 953 \tTrain Loss: 0.2982\n",
      "Epoch 954 \tTrain Loss: 0.3154\n",
      "Epoch 955 \tTrain Loss: 0.2872\n",
      "Epoch 956 \tTrain Loss: 0.2564\n",
      "Epoch 957 \tTrain Loss: 0.2696\n",
      "Epoch 958 \tTrain Loss: 0.2960\n",
      "Epoch 959 \tTrain Loss: 0.2779\n",
      "Epoch 960 \tTrain Loss: 0.2496\n",
      "Epoch 961 \tTrain Loss: 0.2616\n",
      "Epoch 962 \tTrain Loss: 0.2559\n",
      "Epoch 963 \tTrain Loss: 0.2536\n",
      "Epoch 964 \tTrain Loss: 0.2590\n",
      "Epoch 965 \tTrain Loss: 0.2573\n",
      "Epoch 966 \tTrain Loss: 0.2795\n",
      "Epoch 967 \tTrain Loss: 0.2755\n",
      "Epoch 968 \tTrain Loss: 0.2520\n",
      "Epoch 969 \tTrain Loss: 0.2355\n",
      "Epoch 970 \tTrain Loss: 0.2561\n",
      "Epoch 971 \tTrain Loss: 0.2558\n",
      "Epoch 972 \tTrain Loss: 0.2664\n",
      "Epoch 973 \tTrain Loss: 0.2453\n",
      "Epoch 974 \tTrain Loss: 0.2463\n",
      "Epoch 975 \tTrain Loss: 0.2459\n",
      "Epoch 976 \tTrain Loss: 0.2499\n",
      "Epoch 977 \tTrain Loss: 0.2616\n",
      "Epoch 978 \tTrain Loss: 0.2533\n",
      "Epoch 979 \tTrain Loss: 0.2436\n",
      "Epoch 980 \tTrain Loss: 0.2623\n",
      "Epoch 981 \tTrain Loss: 0.2559\n",
      "Epoch 982 \tTrain Loss: 0.2644\n",
      "Epoch 983 \tTrain Loss: 0.2544\n",
      "Epoch 984 \tTrain Loss: 0.2500\n",
      "Epoch 985 \tTrain Loss: 0.2685\n",
      "Epoch 986 \tTrain Loss: 0.2703\n",
      "Epoch 987 \tTrain Loss: 0.2574\n",
      "Epoch 988 \tTrain Loss: 0.2536\n",
      "Epoch 989 \tTrain Loss: 0.2652\n",
      "Epoch 990 \tTrain Loss: 0.2671\n",
      "Epoch 991 \tTrain Loss: 0.2436\n",
      "Epoch 992 \tTrain Loss: 0.2373\n",
      "Epoch 993 \tTrain Loss: 0.2417\n",
      "Epoch 994 \tTrain Loss: 0.2444\n",
      "Epoch 995 \tTrain Loss: 0.2484\n",
      "Epoch 996 \tTrain Loss: 0.2390\n",
      "Epoch 997 \tTrain Loss: 0.2547\n",
      "Epoch 998 \tTrain Loss: 0.2584\n",
      "Epoch 999 \tTrain Loss: 0.2438\n",
      "Epoch 1000 \tTrain Loss: 0.2402\n"
     ]
    }
   ],
   "source": [
    "model = MyMLP(numClass, X_train, y_train, batch_size, hidden_size)\n",
    "model.train(epochs, lamda, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model\n",
      "Batch size: 1000, Hidden Size: 512, Learning Rate: 1\n",
      "\n",
      "Test Accuracy of Overall:   0.97%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy = sum(y_pred == y_test) * 1 / len(y_test)\n",
    "\n",
    "print('Best Model\\nBatch size: {}, Hidden Size: {}, Learning Rate: {}'.format(\n",
    "    batch_size, hidden_size, lr))\n",
    "print('\\nTest Accuracy of Overall:   {:.2f}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
