{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClass = 10\n",
    "batchSize = 64 # 32, 64, 128, 256, 512\n",
    "validSize = 0.2\n",
    "dropOut = 0.2\n",
    "optimizerType = 'RMSprop' # Adam, RMSprop, SGD\n",
    "learningRate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = datasets.MNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testData = datasets.MNIST(root='data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "numTrain, sets = len(trainData), list(range(len(trainData)))\n",
    "np.random.shuffle(sets)\n",
    "\n",
    "splitIndex = int(np.floor(validSize * numTrain))\n",
    "trainIndex, validIndex = sets[splitIndex:], sets[:splitIndex]\n",
    "\n",
    "trainLoader = DataLoader(trainData, batch_size=batchSize, \n",
    "                         sampler=SubsetRandomSampler(trainIndex), num_workers=0)\n",
    "validLoader = DataLoader(trainData, batch_size=batchSize, \n",
    "                         sampler=SubsetRandomSampler(validIndex), num_workers=0)\n",
    "testLoader = DataLoader(testData, batch_size=batchSize, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pytorchMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pytorchMLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 512) # with one hidden layer\n",
    "        self.fc2 = nn.Linear(512, numClass)\n",
    "        self.droput = nn.Dropout(dropOut)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.droput(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pytorchMLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if optimizerType == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    \n",
    "if optimizerType == 'RMSprop':\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learningRate)\n",
    "\n",
    "if optimizerType == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \tTrain Loss: 0.2522 \tValid Loss: 0.0007\n",
      "---------------------------------------------------------------------------------\n",
      "Valid Loss decreased from inf to 0.0007.  Best Model updated and saved.\n",
      "---------------------------------------------------------------------------------\n",
      "Epoch 2 \tTrain Loss: 0.1153 \tValid Loss: 0.0001\n",
      "---------------------------------------------------------------------------------\n",
      "Valid Loss decreased from 0.0007 to 0.0001.  Best Model updated and saved.\n",
      "---------------------------------------------------------------------------------\n",
      "Epoch 3 \tTrain Loss: 0.0782 \tValid Loss: 0.0002\n",
      "Epoch 4 \tTrain Loss: 0.0596 \tValid Loss: 0.0003\n",
      "Epoch 5 \tTrain Loss: 0.0480 \tValid Loss: 0.0000\n",
      "---------------------------------------------------------------------------------\n",
      "Valid Loss decreased from 0.0001 to 0.0000.  Best Model updated and saved.\n",
      "---------------------------------------------------------------------------------\n",
      "Epoch 6 \tTrain Loss: 0.0384 \tValid Loss: 0.0005\n",
      "Epoch 7 \tTrain Loss: 0.0314 \tValid Loss: 0.0004\n",
      "Epoch 8 \tTrain Loss: 0.0285 \tValid Loss: 0.0016\n",
      "Epoch 9 \tTrain Loss: 0.0226 \tValid Loss: 0.0010\n",
      "Early Stopping\n"
     ]
    }
   ],
   "source": [
    "patience, badCounter = 5, 0\n",
    "epochs, validLossMin = 50, np.Inf\n",
    "for epoch in range(epochs):\n",
    "    trainLoss, validLoss = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for data, label in trainLoader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        loss.backward() # to backpropagate the error\n",
    "        optimizer.step()\n",
    "\n",
    "        trainLoss += loss.item() * data.size(0)\n",
    "        \n",
    "    model.eval()\n",
    "    for data, label in validLoader:\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        validLoss = loss.item() * data.size(0)\n",
    "    \n",
    "    trainLoss = trainLoss / len(trainLoader.sampler)\n",
    "    validLoss = validLoss / len(validLoader.sampler)\n",
    "    \n",
    "    print('Epoch {} \\tTrain Loss: {:.4f} \\tValid Loss: {:.4f}'.format(\n",
    "        epoch + 1, trainLoss, validLoss))\n",
    "        \n",
    "    if validLoss < validLossMin:\n",
    "        print('---------------------------------------------------------------------------------')\n",
    "        print('Valid Loss decreased from {:.4f} to {:.4f}.  Best Model updated and saved.'.format(\n",
    "            validLossMin, validLoss))\n",
    "        print('---------------------------------------------------------------------------------')\n",
    "        torch.save(model.state_dict(), 'bestmodel.pt')\n",
    "        validLossMin = validLoss\n",
    "        \n",
    "    else:\n",
    "        badCounter += 1\n",
    "        \n",
    "    if badCounter > patience:\n",
    "        print('Early Stopping')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0679\n",
      "\n",
      "Test Accuracy of  0:   99.08%  (971 / 980)\n",
      "Test Accuracy of  1:   98.94%  (1123 / 1135)\n",
      "Test Accuracy of  2:   98.55%  (1017 / 1032)\n",
      "Test Accuracy of  3:   97.43%  (984 / 1010)\n",
      "Test Accuracy of  4:   98.27%  (965 / 982)\n",
      "Test Accuracy of  5:   97.87%  (873 / 892)\n",
      "Test Accuracy of  6:   96.66%  (926 / 958)\n",
      "Test Accuracy of  7:   97.08%  (998 / 1028)\n",
      "Test Accuracy of  8:   97.74%  (952 / 974)\n",
      "Test Accuracy of  9:   97.52%  (984 / 1009)\n",
      "\n",
      "Best Model\n",
      "Batch size: 64, Valid Size: 0.2\n",
      "Drop out: 0.2, Optimizer: RMSprop, Learning Rate: 0.001\n",
      "\n",
      "Test Accuracy of Overall:   97.93%  (9793 / 10000)\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('bestmodel.pt'))\n",
    "\n",
    "testLoss = 0.0\n",
    "classCorrect = [0. for i in range(numClass)]\n",
    "classTotal = [0. for i in range(numClass)]\n",
    "model.eval()\n",
    "for data, target in testLoader:\n",
    "    output = model(data)\n",
    "\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    testLoss += loss.item() * data.size(0)\n",
    "\n",
    "    __, predictions = torch.max(output, 1)\n",
    "    correct = np.squeeze(predictions.eq(target.data.view_as(predictions)))\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        classCorrect[label] += correct[i].item()\n",
    "        classTotal[label] += 1\n",
    "\n",
    "testLoss = testLoss / len(testLoader.sampler)\n",
    "print('Test Loss: {:.4f}\\n'.format(testLoss))\n",
    "\n",
    "for i in range(numClass):\n",
    "    print('Test Accuracy of {:2d}:   {:.2f}%  ({:.0f} / {:.0f})'.format(\n",
    "        i, 100 * classCorrect[i] / classTotal[i],\n",
    "        np.sum(classCorrect[i]), np.sum(classTotal[i])))\n",
    "\n",
    "print('\\nBest Model\\nBatch size: {}, Valid Size: {}'.format(batchSize, validSize))\n",
    "print('Drop out: {}, Optimizer: {}, Learning Rate: {}'.format(\n",
    "    dropOut, optimizerType, learningRate))\n",
    "print('\\nTest Accuracy of Overall:   {:.2f}%  ({:.0f} / {:.0f})'.format(\n",
    "    100 * np.sum(classCorrect) / np.sum(classTotal),\n",
    "    np.sum(classCorrect), np.sum(classTotal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
